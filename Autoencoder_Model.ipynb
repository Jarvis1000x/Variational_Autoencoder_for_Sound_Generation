{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Autoencoder Model.ipynb",
      "provenance": [],
      "mount_file_id": "1SR9ncUYVnHpx4B0kOTIAQN20HlJr4Agh",
      "authorship_tag": "ABX9TyOt2LEVHNwNTsI613zfqqCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jarvis1000x/Variational_Autoencoder_for_Sound_Generation/blob/main/Autoencoder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cprVUTV74WAH"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
        "    Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class VAE:\n",
        "    \"\"\"\n",
        "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
        "    with mirrored encoder and decoder components.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,\n",
        "                 conv_strides,\n",
        "                 latent_space_dim):\n",
        "        self.input_shape = input_shape # [28, 28, 1]\n",
        "        self.conv_filters = conv_filters # [2, 4, 8]\n",
        "        self.conv_kernels = conv_kernels # [3, 5, 3]\n",
        "        self.conv_strides = conv_strides # [1, 2, 2]\n",
        "        self.latent_space_dim = latent_space_dim # 2\n",
        "        self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.model = None\n",
        "\n",
        "        self._num_conv_layers = len(conv_filters)\n",
        "        self._shape_before_bottleneck = None\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.0001):\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.model.compile(optimizer=optimizer,\n",
        "                           loss=self._calculate_combined_loss)\n",
        "\n",
        "    def train(self, x_train, batch_size, num_epochs):\n",
        "        self.model.fit(x_train,\n",
        "                       x_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       shuffle=True)\n",
        "\n",
        "    def save(self, save_folder=\".\"):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        latent_representations = self.encoder.predict(images)\n",
        "        reconstructed_images = self.decoder.predict(latent_representations)\n",
        "        return reconstructed_images, latent_representations\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, save_folder=\".\"):\n",
        "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        autoencoder = VAE(*parameters)\n",
        "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        autoencoder.load_weights(weights_path)\n",
        "        return autoencoder\n",
        "\n",
        "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                         + kl_loss\n",
        "        return combined_loss\n",
        "\n",
        "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "        error = y_target - y_predicted\n",
        "        reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "        return reconstruction_loss\n",
        "\n",
        "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "        kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                               K.exp(self.log_variance), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "            self.conv_strides,\n",
        "            self.latent_space_dim\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "        self._build_autoencoder()\n",
        "\n",
        "    def _build_autoencoder(self):\n",
        "        model_input = self._model_input\n",
        "        model_output = self.decoder(self.encoder(model_input))\n",
        "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        decoder_input = self._add_decoder_input()\n",
        "        dense_layer = self._add_dense_layer(decoder_input)\n",
        "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "    def _add_decoder_input(self):\n",
        "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "    def _add_dense_layer(self, decoder_input):\n",
        "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
        "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "        return dense_layer\n",
        "\n",
        "    def _add_reshape_layer(self, dense_layer):\n",
        "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "    def _add_conv_transpose_layers(self, x):\n",
        "        \"\"\"Add conv transpose blocks.\"\"\"\n",
        "        # loop through all the conv layers in reverse order and stop at the\n",
        "        # first layer\n",
        "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "            x = self._add_conv_transpose_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_transpose_layer(self, layer_index, x):\n",
        "        layer_num = self._num_conv_layers - layer_index\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_decoder_output(self, x):\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=self.conv_kernels[0],\n",
        "            strides=self.conv_strides[0],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "        return output_layer\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        encoder_input = self._add_encoder_input()\n",
        "        conv_layers = self._add_conv_layers(encoder_input)\n",
        "        bottleneck = self._add_bottleneck(conv_layers)\n",
        "        self._model_input = encoder_input\n",
        "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "    def _add_encoder_input(self):\n",
        "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "    def _add_conv_layers(self, encoder_input):\n",
        "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
        "        x = encoder_input\n",
        "        for layer_index in range(self._num_conv_layers):\n",
        "            x = self._add_conv_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_layer(self, layer_index, x):\n",
        "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
        "        conv 2d + ReLU + batch normalization.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        conv_layer = Conv2D(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"encoder_conv_layer_{layer_number}\"\n",
        "        )\n",
        "        x = conv_layer(x)\n",
        "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_bottleneck(self, x):\n",
        "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
        "        layer).\n",
        "        \"\"\"\n",
        "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "        x = Flatten()(x)\n",
        "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "        self.log_variance = Dense(self.latent_space_dim,\n",
        "                                  name=\"log_variance\")(x)\n",
        "\n",
        "        def sample_point_from_normal_distribution(args):\n",
        "            mu, log_variance = args\n",
        "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                      stddev=1.)\n",
        "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "            return sampled_point\n",
        "\n",
        "        x = Lambda(sample_point_from_normal_distribution,\n",
        "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WP7THfr4kir"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 200\n",
        "\n",
        "SPECTROGRAMS_PATH = \"/content/drive/MyDrive/Colab Notebooks/Sound Generation using VAE/data/spectrograms\"\n",
        "\n",
        "\n",
        "def load_fsdd(spectrograms_path):\n",
        "    x_train = []\n",
        "    for root, _, file_names in os.walk(spectrograms_path):\n",
        "        for file_name in file_names:\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            spectrogram = np.load(file_path, allow_pickle=True) # (n_bins, n_frames, 1)\n",
        "            x_train.append(spectrogram)\n",
        "    x_train = np.array(x_train)\n",
        "    x_train = x_train[..., np.newaxis] # -> (3000, 256, 64, 1)\n",
        "    return x_train\n",
        "\n",
        "\n",
        "def train(x_train, learning_rate, batch_size, epochs):\n",
        "    autoencoder = VAE(\n",
        "        input_shape=(256, 64, 1),\n",
        "        conv_filters=(512, 256, 128, 64, 32),\n",
        "        conv_kernels=(3, 3, 3, 3, 3),\n",
        "        conv_strides=(2, 2, 2, 2, (2, 1)),\n",
        "        latent_space_dim=128\n",
        "    )\n",
        "    autoencoder.summary()\n",
        "    autoencoder.compile(learning_rate)\n",
        "    autoencoder.train(x_train, batch_size, epochs)\n",
        "    return autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K04wmn745LV",
        "outputId": "60465221-ae46-4ba9-d5ed-a0d97d5680ef"
      },
      "source": [
        "x_train = load_fsdd(SPECTROGRAMS_PATH)\n",
        "autoencoder = train(x_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
        "autoencoder.save(\"model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 256, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_1 (Conv2D)   (None, 128, 32, 512) 5120        encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_1 (ReLU)           (None, 128, 32, 512) 0           encoder_conv_layer_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_1 (BatchNormalizatio (None, 128, 32, 512) 2048        encoder_relu_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_2 (Conv2D)   (None, 64, 16, 256)  1179904     encoder_bn_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_2 (ReLU)           (None, 64, 16, 256)  0           encoder_conv_layer_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_2 (BatchNormalizatio (None, 64, 16, 256)  1024        encoder_relu_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_3 (Conv2D)   (None, 32, 8, 128)   295040      encoder_bn_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_3 (ReLU)           (None, 32, 8, 128)   0           encoder_conv_layer_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_3 (BatchNormalizatio (None, 32, 8, 128)   512         encoder_relu_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_4 (Conv2D)   (None, 16, 4, 64)    73792       encoder_bn_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_4 (ReLU)           (None, 16, 4, 64)    0           encoder_conv_layer_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_4 (BatchNormalizatio (None, 16, 4, 64)    256         encoder_relu_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_layer_5 (Conv2D)   (None, 8, 4, 32)     18464       encoder_bn_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_relu_5 (ReLU)           (None, 8, 4, 32)     0           encoder_conv_layer_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_bn_5 (BatchNormalizatio (None, 8, 4, 32)     128         encoder_relu_5[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1024)         0           encoder_bn_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 128)          131200      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "log_variance (Dense)            (None, 128)          131200      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 128)          0           mu[0][0]                         \n",
            "                                                                 log_variance[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,838,688\n",
            "Trainable params: 1,836,704\n",
            "Non-trainable params: 1,984\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "decoder_input (InputLayer)   [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "decoder_dense (Dense)        (None, 1024)              132096    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 8, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 16, 4, 32)         9248      \n",
            "_________________________________________________________________\n",
            "decoder_relu_1 (ReLU)        (None, 16, 4, 32)         0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_1 (BatchNormaliza (None, 16, 4, 32)         128       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 32, 8, 64)         18496     \n",
            "_________________________________________________________________\n",
            "decoder_relu_2 (ReLU)        (None, 32, 8, 64)         0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_2 (BatchNormaliza (None, 32, 8, 64)         256       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 64, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "decoder_relu_3 (ReLU)        (None, 64, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_3 (BatchNormaliza (None, 64, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 128, 32, 256)      295168    \n",
            "_________________________________________________________________\n",
            "decoder_relu_4 (ReLU)        (None, 128, 32, 256)      0         \n",
            "_________________________________________________________________\n",
            "decoder_bn_4 (BatchNormaliza (None, 128, 32, 256)      1024      \n",
            "_________________________________________________________________\n",
            "decoder_conv_transpose_layer (None, 256, 64, 1)        2305      \n",
            "_________________________________________________________________\n",
            "sigmoid_layer (Activation)   (None, 256, 64, 1)        0         \n",
            "=================================================================\n",
            "Total params: 533,089\n",
            "Trainable params: 532,129\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 256, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "encoder (Functional)         (None, 128)               1838688   \n",
            "_________________________________________________________________\n",
            "decoder (Functional)         (None, 256, 64, 1)        533089    \n",
            "=================================================================\n",
            "Total params: 2,371,777\n",
            "Trainable params: 2,368,833\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n",
            "Train on 3000 samples\n",
            "Epoch 1/200\n",
            "3000/3000 [==============================] - 69s 23ms/sample - loss: 109502.5156\n",
            "Epoch 2/200\n",
            "3000/3000 [==============================] - 27s 9ms/sample - loss: 33537.4916\n",
            "Epoch 3/200\n",
            "3000/3000 [==============================] - 27s 9ms/sample - loss: 27768.3417\n",
            "Epoch 4/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 23383.7466\n",
            "Epoch 5/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21949.5045\n",
            "Epoch 6/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21679.4429\n",
            "Epoch 7/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21501.5986\n",
            "Epoch 8/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21221.8559\n",
            "Epoch 9/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21100.9614\n",
            "Epoch 10/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 21045.0502\n",
            "Epoch 11/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 20921.2731\n",
            "Epoch 12/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 20954.8205\n",
            "Epoch 13/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 20828.2449\n",
            "Epoch 14/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 20803.5188\n",
            "Epoch 15/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 20188.5109\n",
            "Epoch 16/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 13813.3012\n",
            "Epoch 17/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 10323.7756\n",
            "Epoch 18/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9846.8481\n",
            "Epoch 19/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9610.3806\n",
            "Epoch 20/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9571.0991\n",
            "Epoch 21/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9466.8182\n",
            "Epoch 22/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9339.5031\n",
            "Epoch 23/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9341.1361\n",
            "Epoch 24/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9262.7958\n",
            "Epoch 25/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9232.3105\n",
            "Epoch 26/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9178.3455\n",
            "Epoch 27/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9137.9653\n",
            "Epoch 28/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9112.7409\n",
            "Epoch 29/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9052.4204\n",
            "Epoch 30/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9068.8530\n",
            "Epoch 31/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9061.5531\n",
            "Epoch 32/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8984.0491\n",
            "Epoch 33/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8957.4475\n",
            "Epoch 34/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8992.5314\n",
            "Epoch 35/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 9002.1420\n",
            "Epoch 36/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8940.7098\n",
            "Epoch 37/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8928.2372\n",
            "Epoch 38/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8879.0552\n",
            "Epoch 39/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8866.5174\n",
            "Epoch 40/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8893.1205\n",
            "Epoch 41/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8833.7173\n",
            "Epoch 42/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8896.3792\n",
            "Epoch 43/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8780.6458\n",
            "Epoch 44/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8901.5343\n",
            "Epoch 45/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8767.9134\n",
            "Epoch 46/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8745.1328\n",
            "Epoch 47/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8735.0134\n",
            "Epoch 48/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8760.5250\n",
            "Epoch 49/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8740.4277\n",
            "Epoch 50/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8734.0439\n",
            "Epoch 51/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8697.6631\n",
            "Epoch 52/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8730.1121\n",
            "Epoch 53/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8707.4330\n",
            "Epoch 54/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8699.4765\n",
            "Epoch 55/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8690.2992\n",
            "Epoch 56/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8687.5267\n",
            "Epoch 57/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 8590.9030\n",
            "Epoch 58/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 7517.9348\n",
            "Epoch 59/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 5235.4946\n",
            "Epoch 60/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3938.6686\n",
            "Epoch 61/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3815.6909\n",
            "Epoch 62/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3734.6807\n",
            "Epoch 63/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3703.9955\n",
            "Epoch 64/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3653.9642\n",
            "Epoch 65/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3749.8850\n",
            "Epoch 66/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3693.5616\n",
            "Epoch 67/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3603.2561\n",
            "Epoch 68/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3664.7632\n",
            "Epoch 69/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3599.2258\n",
            "Epoch 70/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3655.2254\n",
            "Epoch 71/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3607.4616\n",
            "Epoch 72/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3570.8674\n",
            "Epoch 73/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3578.0517\n",
            "Epoch 74/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3582.0221\n",
            "Epoch 75/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3636.2171\n",
            "Epoch 76/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3549.9256\n",
            "Epoch 77/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3564.3836\n",
            "Epoch 78/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3546.5967\n",
            "Epoch 79/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3506.4403\n",
            "Epoch 80/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3514.8988\n",
            "Epoch 81/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3486.3886\n",
            "Epoch 82/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3527.0224\n",
            "Epoch 83/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3473.6088\n",
            "Epoch 84/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3481.9061\n",
            "Epoch 85/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3591.8730\n",
            "Epoch 86/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3499.4845\n",
            "Epoch 87/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3468.4584\n",
            "Epoch 88/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3501.7032\n",
            "Epoch 89/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3445.1372\n",
            "Epoch 90/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3462.1795\n",
            "Epoch 91/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3441.5213\n",
            "Epoch 92/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3427.9456\n",
            "Epoch 93/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3441.7975\n",
            "Epoch 94/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3459.1453\n",
            "Epoch 95/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3412.8523\n",
            "Epoch 96/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3433.5491\n",
            "Epoch 97/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3423.7801\n",
            "Epoch 98/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3433.9037\n",
            "Epoch 99/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3450.1915\n",
            "Epoch 100/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3419.7929\n",
            "Epoch 101/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3409.9062\n",
            "Epoch 102/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3391.9066\n",
            "Epoch 103/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3385.2391\n",
            "Epoch 104/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3404.8881\n",
            "Epoch 105/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3418.8531\n",
            "Epoch 106/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3389.3661\n",
            "Epoch 107/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3412.8804\n",
            "Epoch 108/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3396.7357\n",
            "Epoch 109/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3400.1138\n",
            "Epoch 110/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3378.2980\n",
            "Epoch 111/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3359.0465\n",
            "Epoch 112/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3368.6884\n",
            "Epoch 113/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3432.9923\n",
            "Epoch 114/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3351.1016\n",
            "Epoch 115/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3351.8563\n",
            "Epoch 116/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3364.8829\n",
            "Epoch 117/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3382.8371\n",
            "Epoch 118/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3318.6272\n",
            "Epoch 119/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3343.9183\n",
            "Epoch 120/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3327.7800\n",
            "Epoch 121/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3314.8042\n",
            "Epoch 122/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3320.7624\n",
            "Epoch 123/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3382.7320\n",
            "Epoch 124/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3349.5069\n",
            "Epoch 125/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3388.3508\n",
            "Epoch 126/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3350.1754\n",
            "Epoch 127/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3390.8936\n",
            "Epoch 128/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3326.2854\n",
            "Epoch 129/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3326.8948\n",
            "Epoch 130/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3314.6040\n",
            "Epoch 131/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3302.1779\n",
            "Epoch 132/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3321.4671\n",
            "Epoch 133/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3319.2040\n",
            "Epoch 134/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3295.2540\n",
            "Epoch 135/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3321.6942\n",
            "Epoch 136/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3297.7008\n",
            "Epoch 137/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3298.7658\n",
            "Epoch 138/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3293.9566\n",
            "Epoch 139/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3319.6891\n",
            "Epoch 140/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3331.1562\n",
            "Epoch 141/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3276.1400\n",
            "Epoch 142/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3297.3006\n",
            "Epoch 143/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3252.9915\n",
            "Epoch 144/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3261.7243\n",
            "Epoch 145/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3300.1109\n",
            "Epoch 146/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3281.6061\n",
            "Epoch 147/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3269.1834\n",
            "Epoch 148/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3249.6506\n",
            "Epoch 149/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3256.2819\n",
            "Epoch 150/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3333.9578\n",
            "Epoch 151/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3248.6868\n",
            "Epoch 152/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3300.1821\n",
            "Epoch 153/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3296.1509\n",
            "Epoch 154/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3247.0101\n",
            "Epoch 155/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3242.8444\n",
            "Epoch 156/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3255.9761\n",
            "Epoch 157/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3234.9582\n",
            "Epoch 158/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3239.2528\n",
            "Epoch 159/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3307.0341\n",
            "Epoch 160/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3240.2681\n",
            "Epoch 161/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3246.2488\n",
            "Epoch 162/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3222.4008\n",
            "Epoch 163/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3211.3338\n",
            "Epoch 164/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3243.8917\n",
            "Epoch 165/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3232.4947\n",
            "Epoch 166/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3305.0088\n",
            "Epoch 167/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3237.6115\n",
            "Epoch 168/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3220.6170\n",
            "Epoch 169/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3209.2023\n",
            "Epoch 170/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3269.3497\n",
            "Epoch 171/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3202.8272\n",
            "Epoch 172/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3222.0658\n",
            "Epoch 173/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3220.4395\n",
            "Epoch 174/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3212.5540\n",
            "Epoch 175/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3267.4393\n",
            "Epoch 176/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3226.8186\n",
            "Epoch 177/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3201.5319\n",
            "Epoch 178/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3200.3264\n",
            "Epoch 179/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3201.9782\n",
            "Epoch 180/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3186.4267\n",
            "Epoch 181/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3190.8625\n",
            "Epoch 182/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3215.5600\n",
            "Epoch 183/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3185.3937\n",
            "Epoch 184/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3173.1893\n",
            "Epoch 185/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3171.7302\n",
            "Epoch 186/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3188.6765\n",
            "Epoch 187/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3232.8355\n",
            "Epoch 188/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3206.4374\n",
            "Epoch 189/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3175.1803\n",
            "Epoch 190/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3177.4808\n",
            "Epoch 191/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3169.4162\n",
            "Epoch 192/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3206.8009\n",
            "Epoch 193/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3203.1959\n",
            "Epoch 194/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3162.6107\n",
            "Epoch 195/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3154.6829\n",
            "Epoch 196/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3160.0695\n",
            "Epoch 197/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3162.2996\n",
            "Epoch 198/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3157.8966\n",
            "Epoch 199/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3153.4740\n",
            "Epoch 200/200\n",
            "3000/3000 [==============================] - 28s 9ms/sample - loss: 3149.0779\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}